{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87994a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from lib.src.rl.utils.utils import Utils\n",
    "from lib.src.rl.utils.visualize import Visualization as Vis\n",
    "from lib.src.rl.agents.q_learning import QLearningAgent\n",
    "from lib.src.rl.agents.value_iteration import ValueIterationAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b9d3f",
   "metadata": {},
   "source": [
    "# CliffWalking-v0\n",
    "\n",
    "![Cliff Walking by OpenAI Gym](./img/cliff_walking.gif)\n",
    "\n",
    "## State space\n",
    "\n",
    "- A $4 \\times 12$ grid world\n",
    "- Positions represented as flattened index\n",
    "  - For example, the starting point (3, 0) is represented as $3 \\times 12 + 0 = 36$\n",
    "\n",
    "## Action space\n",
    "- 0: Move up\n",
    "- 1: Move right\n",
    "- 2: Move down\n",
    "- 3: Move left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b59866",
   "metadata": {},
   "source": [
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1aa36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, {'prob': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f4c28",
   "metadata": {},
   "source": [
    "## Value Iteration Agent Demo\n",
    "- set the stopping critera $\\theta = 0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7549ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ValueIterationAgent(env, \"CliffWalking-v0\")\n",
    "policy = v.value_iteration(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2765f0",
   "metadata": {},
   "source": [
    "### Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5024e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<╔════════════╗\n",
      " ║→→→→→→→→→→→↓║\n",
      " ║→→→→→→→→→→→↓║\n",
      " ║→→→→→→→→→→→↓║\n",
      " ║↑▀▀▀▀▀▀▀▀▀▀☺║\n",
      " ╚════════════╝>\n"
     ]
    }
   ],
   "source": [
    "Vis.visualize(\"CliffWalking-v0\", env.shape, v.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4ad87",
   "metadata": {},
   "source": [
    "## Q-Learning Agent Demo\n",
    "\n",
    "- set agent_info\n",
    "  - the implementation of the Q-learning agent is slightly different from the implementation of the Value Iteration agent, so we're passing arguments in a dictionary to the constructor of the ```QLearningAgent``` class in addition to the ```env``` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504184f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_info = {\n",
    "    \"num_actions\": env.action_space.n, \n",
    "    \"num_states\": env.observation_space.n, \n",
    "    \"epsilon\": 0.1, \n",
    "    \"alpha\": 0.5, \n",
    "    \"gamma\": 1.0\n",
    "}\n",
    "q = QLearningAgent(env, \"CliffWalking-v0\", agent_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddcab6a",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "- A Q-Learning agent directly learns the value of state-action pairs, while a Value Iteration agent learns state-value functions and greedifies the policy with regard to the estimation of the state-value\n",
    "- A Q-Learning agent combines the advantages of a Dynamic Programming method and a Monte Carlo method\n",
    "  - It boostraps: learns a guess from a guess\n",
    "  - It does not require the model of the environment (the transition dynamic)\n",
    "    - It learns by directly interaction with the environment\n",
    "- Thus, while for a Value Iteration agent, we use the transition dynamics of the environment to compute the state values (i.e., planning), for a Q-Learning agent, we need to interact with the environment for some episodes to learn (i.e., learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1414a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 100\n",
    "for i in range(num_episode):\n",
    "    observation, info = env.reset()\n",
    "    q.agent_step(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e91f9b",
   "metadata": {},
   "source": [
    "### Visualize the result\n",
    "- For a Value Iteration agent, we pass its learned policy to the ```visualize()``` function to display the optimal route the agent should take\n",
    "- For a Q-Learning agent, we only learned the ```q``` value, which is enough for us to figure out the policy\n",
    "  - Since a q-value represents the value of a state-action pair, to figure out the learned policy, in each state, we can simply select the action that results in the largest value of the state-action pair\n",
    "    - It turns out that in my implementation, the q-value list (a 2D list) for a Q-Learning agent and the policy list (also a 2D list) for a Value Iteration agent can both be used directly to draw the route of the agent without any modification\n",
    "      - Simply use ```np.argmax()``` to decide the arrow in each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f82a4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<╔════════════╗\n",
      " ║→↓→→↓→←→↓→→↓║\n",
      " ║←→↓→↑→←↓↓↓↓↓║\n",
      " ║→→→→→→→→→→→↓║\n",
      " ║↑▀▀▀▀▀▀▀▀▀▀☺║\n",
      " ╚════════════╝>\n"
     ]
    }
   ],
   "source": [
    "Vis.visualize(\"CliffWalking-v0\", env.shape, q.q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
